{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tiny_Image+Yolo.ipynb","provenance":[{"file_id":"https://github.com/sonugiri1043/Train_ResNet_On_Tiny_ImageNet/blob/master/Train_ResNet_On_Tiny_ImageNet.ipynb","timestamp":1603014862601}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"2399de4a4bec4d4bacb122b6f0ff3610":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_268a63dc4d854356a088bdef02a5fa01","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_42a32cf1992d4d37854f641a8aa8295c","IPY_MODEL_73e398b3870c4796819aa058a2366b5b"]}},"268a63dc4d854356a088bdef02a5fa01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"42a32cf1992d4d37854f641a8aa8295c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_96f8d4f466f54da1bbabd80c413c9a8e","_dom_classes":[],"description":"Loading train folder...: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":200,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":200,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0a6acf15a0404a1697ce81dc518531fc"}},"73e398b3870c4796819aa058a2366b5b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4f97c4ed3740425980c74d147acb3d90","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 200/200 [00:00&lt;00:00, 1040.98it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7bd5bf7270464058b8f0f90b4a01b605"}},"96f8d4f466f54da1bbabd80c413c9a8e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0a6acf15a0404a1697ce81dc518531fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4f97c4ed3740425980c74d147acb3d90":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7bd5bf7270464058b8f0f90b4a01b605":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"881cb1fd1d5444c2812fd888e712dfd0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5aad7a8265f749ee909a7fd76d693ba7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7ab3b589e0044a55aaac56687857d080","IPY_MODEL_a8791f9c108a4b99b97dd88ff8eb5b47"]}},"5aad7a8265f749ee909a7fd76d693ba7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7ab3b589e0044a55aaac56687857d080":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_baaed7e521e94ad6a08c81715f7c4434","_dom_classes":[],"description":"Loading val folder....: ","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_20b3a81ce5664b0eb5d2b60eb981c6a2"}},"a8791f9c108a4b99b97dd88ff8eb5b47":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_500eeda9167141039f193aa7d8e0985b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 10000/? [00:00&lt;00:00, 129542.59it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_141090c1d7654d9eb4922f3154df2098"}},"baaed7e521e94ad6a08c81715f7c4434":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"20b3a81ce5664b0eb5d2b60eb981c6a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"500eeda9167141039f193aa7d8e0985b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"141090c1d7654d9eb4922f3154df2098":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"lilnm_RyBnAD"},"source":["## Sonu Giri\n","> contact: sonugiri1043@gmail.com\n","\n","- DataSet: Tiny ImageNet\n","- Model: ResNet-50 ( Without Fully Connected layer )"]},{"cell_type":"markdown","metadata":{"id":"3ic3Mc6ioXdm"},"source":["Tiny ImageNet dataset consists of 200 categories and each category has 500 of 64x64 size images in training set."]},{"cell_type":"markdown","metadata":{"id":"lcZtlW9OyMQ0"},"source":["## Load Tiny ImageNet DataSet"]},{"cell_type":"code","metadata":{"id":"C4Mn6-kHncmV","executionInfo":{"status":"ok","timestamp":1603033565005,"user_tz":-330,"elapsed":2438,"user":{"displayName":"Prasad Shripathi","photoUrl":"","userId":"01122758420514335828"}},"outputId":"af541147-a444-43c8-f5ed-656aba43c452","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# load Google Driver\n","from google.colab import drive\n","drive.mount('/content/drive',force_remount=True)\n","my_path = '/content/drive/My Drive/Colab Notebooks/EVA 5/12.Yolo+Tinyimagenet/'"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RntLE9efqLIm","executionInfo":{"status":"ok","timestamp":1603033568445,"user_tz":-330,"elapsed":1050,"user":{"displayName":"Prasad Shripathi","photoUrl":"","userId":"01122758420514335828"}},"outputId":"b062f824-4d75-47e1-c128-c64d9b1d56b9","colab":{"base_uri":"https://localhost:8080/","height":329}},"source":["!rm -rf '/content/drive/My Drive/Colab Notebooks/EVA 5/12.Yolo+Tinyimagenet/__pycache__'\n","!ls -lrt '/content/drive/My Drive/Colab Notebooks/EVA 5/12.Yolo+Tinyimagenet/'"],"execution_count":4,"outputs":[{"output_type":"stream","text":["total 242907\n","-rw------- 1 root root 248100043 Oct 18 07:54  tiny-imagenet-200.zip\n","-rw------- 1 root root      2132 Oct 18 08:35  dataAlbumentationLoad.py\n","-rw------- 1 root root      4849 Oct 18 08:35  dataTorchTransformLoad.py\n","-rw------- 1 root root      3051 Oct 18 08:35  DH_NET.py\n","-rw------- 1 root root      5882 Oct 18 08:35  GradCam.py\n","-rw------- 1 root root     31322 Oct 18 08:35  grid_image.png\n","-rw------- 1 root root      3657 Oct 18 08:35  Plots.py\n","-rw------- 1 root root      4077 Oct 18 08:36  QuizDNN.py\n","-rw------- 1 root root       575 Oct 18 08:36  README.md\n","-rw------- 1 root root      5380 Oct 18 08:36  Test_Train.py\n","-rw------- 1 root root      3694 Oct 18 08:36  VS_NET.py\n","-rw------- 1 root root      5554 Oct 18 08:37  Test_Train_OCP.py\n","drwx------ 2 root root      4096 Oct 18 08:37  outputs\n","drwx------ 2 root root      4096 Oct 18 10:42  tiny-imagenet-200\n","-rw------- 1 root root    427114 Oct 18 14:10  Tiny_Imagenet.ipynb\n","-rw------- 1 root root      4496 Oct 18 14:58  Resnet.py\n","-rw------- 1 root root    122854 Oct 18 15:05 'Copy of Sonu_ResNet-50_51_Accu_IMG_AUG_DEFAULT.ipynb'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lK5c1V7qLg4x","executionInfo":{"status":"ok","timestamp":1603033573009,"user_tz":-330,"elapsed":1139,"user":{"displayName":"Prasad Shripathi","photoUrl":"","userId":"01122758420514335828"}}},"source":["# import\n","\n","from __future__ import print_function\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","# import torch.functional as F\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms\n","import torchvision\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from torchsummary import summary\n","from tqdm import tqdm\n","from torch.optim.lr_scheduler import StepLR,ReduceLROnPlateau\n","import sys\n","import random\n","import matplotlib"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"vIAvBiwVLkyx","executionInfo":{"status":"ok","timestamp":1603033575391,"user_tz":-330,"elapsed":1020,"user":{"displayName":"Prasad Shripathi","photoUrl":"","userId":"01122758420514335828"}}},"source":["sys.path.append('/content/drive/My Drive/Colab Notebooks/EVA 5/12.Yolo+Tinyimagenet/')"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"49bKGR24Lny0","executionInfo":{"status":"ok","timestamp":1603033583936,"user_tz":-330,"elapsed":1446,"user":{"displayName":"Prasad Shripathi","photoUrl":"","userId":"01122758420514335828"}}},"source":["# import for all the modular codes\n","import VS_NET\n","import Test_Train\n","import Test_Train_OCP\n","import Plots\n","import dataTorchTransformLoad as dtl\n","import Resnet\n","import GradCam\n","import Resnet\n","import dataAlbumentationLoad as dal\n","import GradCam\n","from GradCam import GradCAM, visualize_cam, save_misclassified, download_img_pil,pil_img_transform\n","from Plots import plot_misclassified,imshow\n","import DH_NET"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"GPT7prZ0Lq42","executionInfo":{"status":"ok","timestamp":1603034094461,"user_tz":-330,"elapsed":1014,"user":{"displayName":"Prasad Shripathi","photoUrl":"","userId":"01122758420514335828"}},"outputId":"c13aba9d-614c-4087-c5dd-c83739982b7a","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["my_path"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/Colab Notebooks/EVA 5/12.Yolo+Tinyimagenet/'"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"OIErilEdN0_j","executionInfo":{"status":"ok","timestamp":1603020751940,"user_tz":-330,"elapsed":1047,"user":{"displayName":"Prasad Shripathi","photoUrl":"","userId":"01122758420514335828"}}},"source":["from scipy.ndimage import imread"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"-U9DhN2hN-W7","executionInfo":{"status":"ok","timestamp":1603017870392,"user_tz":-330,"elapsed":1190631,"user":{"displayName":"Prasad Shripathi","photoUrl":"","userId":"01122758420514335828"}},"outputId":"0a42cdab-c12b-4392-a3a9-a17b61df4311","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["import zipfile\n","\n","path_to_zip_file = \"/content/drive/My Drive/Colab Notebooks/EVA 5/12.Yolo+Tinyimagenet/tiny-imagenet-200.zip\"\n","directory_to_extract_to = '/content/drive/My Drive/Colab Notebooks/EVA 5/12.Yolo+Tinyimagenet/'\n","print(\"Extracting zip file: %s\" % path_to_zip_file)\n","with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n","  zip_ref.extractall(directory_to_extract_to)\n","print(\"Extracted at: %s\" % directory_to_extract_to)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Extracting zip file: /content/drive/My Drive/Colab Notebooks/EVA 5/12.Yolo+Tinyimagenet/tiny-imagenet-200.zip\n","Extracted at: /content/drive/My Drive/Colab Notebooks/EVA 5/12.Yolo+Tinyimagenet/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sjVmkjZwLtS7","executionInfo":{"status":"ok","timestamp":1603016174632,"user_tz":-330,"elapsed":16682,"user":{"displayName":"Prasad Shripathi","photoUrl":"","userId":"01122758420514335828"}},"outputId":"fb229252-865c-43a1-dd0f-3975f4ada79d","colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["! git clone https://github.com/seshuad/IMagenet\n","! ls 'IMagenet/tiny-imagenet-200/'"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Cloning into 'IMagenet'...\n","remote: Enumerating objects: 120594, done.\u001b[K\n","remote: Total 120594 (delta 0), reused 0 (delta 0), pack-reused 120594\u001b[K\n","Receiving objects: 100% (120594/120594), 212.68 MiB | 30.01 MiB/s, done.\n","Resolving deltas: 100% (1115/1115), done.\n","Checking out files: 100% (120206/120206), done.\n","test  train  val  wnids.txt  words.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wRZagZn0dwUd","executionInfo":{"status":"ok","timestamp":1603020920106,"user_tz":-330,"elapsed":10933,"user":{"displayName":"Prasad Shripathi","photoUrl":"","userId":"01122758420514335828"}},"outputId":"4b71573b-7a9e-4766-c694-0c14418f93b3","colab":{"base_uri":"https://localhost:8080/","height":384}},"source":["!pip uninstall scipy\n","\n","!pip install scipy==1.1.0"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Uninstalling scipy-1.1.0:\n","  Would remove:\n","    /usr/local/lib/python3.6/dist-packages/scipy-1.1.0.dist-info/*\n","    /usr/local/lib/python3.6/dist-packages/scipy/*\n","Proceed (y/n)? y\n","  Successfully uninstalled scipy-1.1.0\n","Collecting scipy==1.1.0\n","  Using cached https://files.pythonhosted.org/packages/a8/0b/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl\n","Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.1.0) (1.18.5)\n","\u001b[31mERROR: umap-learn 0.4.6 has requirement scipy>=1.3.1, but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.3.0 has requirement scipy==1.4.1, but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: plotnine 0.6.0 has requirement scipy>=1.2.0, but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: scipy\n","Successfully installed scipy-1.1.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["scipy"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"PHi2yLx9fx62"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MPRltIc4zcT4"},"source":["## Extract test and training data from Tiny ImageNet DataSet"]},{"cell_type":"code","metadata":{"id":"7TUH7bu7n5ta","executionInfo":{"status":"ok","timestamp":1603033593801,"user_tz":-330,"elapsed":1443,"user":{"displayName":"Prasad Shripathi","photoUrl":"","userId":"01122758420514335828"}}},"source":["from torch.utils.data import Dataset\n","from PIL import Image\n","import numpy as np\n","from tqdm import notebook\n","import random\n","import pdb\n","\n","\n","class ProcessTinyImageNet():\n","\n","    path = '/content/drive/My Drive/Colab Notebooks/EVA 5/12.Yolo+Tinyimagenet//tiny-imagenet-200/'\n","\n","    def __init__(self, classes_map):\n","        self.image = []\n","        self.target = []\n","        self.classes = classes_map\n","        self.update_train_val_data()\n","\n","    @staticmethod\n","    def get_train_image_ids():\n","        \"\"\"\n","            returns: ['n02321529',...]\n","        \"\"\"\n","        path = ProcessTinyImageNet.path + \"wnids.txt\"\n","        train_image_ids = [line.strip() for line in open(path)]\n","        return train_image_ids\n","\n","    @staticmethod\n","    def all_class_map():\n","        \"\"\"\n","        eg:\n","        {'n02423022': 'label1'}\n","        \"\"\"\n","        all_class_map = {}\n","        windis = ProcessTinyImageNet.get_train_image_ids()\n","        for line in open(ProcessTinyImageNet.path + \"words.txt\", 'r'):\n","            data = line.split('\\t')[:2]\n","            n_id = data[0]\n","            class_name = data[1].strip()\n","            if n_id in windis:\n","              all_class_map[n_id] = class_name\n","        return all_class_map\n","\n","    @staticmethod\n","    def get_image(image_path):\n","        img = Image.open(image_path)\n","        npimg = np.asarray(img)\n","        if len(npimg.shape) == 2:\n","            npimg = np.repeat(npimg[:, :, np.newaxis], 3, axis=2)\n","        return npimg\n","\n","    def update_train_data(self):\n","        \"\"\"\n","        returns: {'train/n02124075/images/n02124075_10.JPEG': 'Egyptian cat'}\n","        \"\"\"\n","        image_ids = ProcessTinyImageNet.get_train_image_ids()\n","        class_map = self.classes\n","        train_image_path = ProcessTinyImageNet.path + \"train/{image_id}/images/{image_name}.JPEG\"\n","        for image_id in notebook.tqdm(image_ids, desc='Loading train folder...'):\n","            for key_id in range(500):\n","                image_name = image_id + \"_\" + str(key_id)\n","                train_path = train_image_path.format(image_id=image_id, image_name=image_name)\n","                #npimg = ProcessTinyImageNet.get_image(train_path)\n","                train_label = class_map[image_id]\n","                self.image.append(train_path)\n","                self.target.append(train_label)\n","\n","    def update_val_data(self):\n","        \"\"\"\n","        returns: {'val/images/val_10.JPEG': 'swimming trunks, bathing trunks'}\n","        \"\"\"\n","        path = ProcessTinyImageNet.path + \"val/val_annotations.txt\"\n","        class_map = self.classes\n","        val_image_path = ProcessTinyImageNet.path + \"val/images/{image_name}\"\n","        for line in notebook.tqdm(open(path, 'r'), desc='Loading val folder....'):\n","            data = line.split('\\t')[:2]\n","            image_name = data[0].strip()\n","            image_id = data[1].strip()\n","            image_path = val_image_path.format(image_name=image_name)\n","            #npimg = ProcessTinyImageNet.get_image(image_path)\n","            val_label = class_map[image_id]\n","            self.image.append(image_path)\n","            self.target.append(val_label)\n","\n","    def update_train_val_data(self):\n","        self.update_train_data()\n","        self.update_val_data()"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"97x8o4cTHfxp"},"source":["## Shuffle training data"]},{"cell_type":"code","metadata":{"id":"K2kPgxn_Hd4n","executionInfo":{"status":"ok","timestamp":1603033597423,"user_tz":-330,"elapsed":1369,"user":{"displayName":"Prasad Shripathi","photoUrl":"","userId":"01122758420514335828"}},"outputId":"8d512163-bac9-4dd0-cdcb-b161d2ff14e8","colab":{"base_uri":"https://localhost:8080/","height":133,"referenced_widgets":["2399de4a4bec4d4bacb122b6f0ff3610","268a63dc4d854356a088bdef02a5fa01","42a32cf1992d4d37854f641a8aa8295c","73e398b3870c4796819aa058a2366b5b","96f8d4f466f54da1bbabd80c413c9a8e","0a6acf15a0404a1697ce81dc518531fc","4f97c4ed3740425980c74d147acb3d90","7bd5bf7270464058b8f0f90b4a01b605","881cb1fd1d5444c2812fd888e712dfd0","5aad7a8265f749ee909a7fd76d693ba7","7ab3b589e0044a55aaac56687857d080","a8791f9c108a4b99b97dd88ff8eb5b47","baaed7e521e94ad6a08c81715f7c4434","20b3a81ce5664b0eb5d2b60eb981c6a2","500eeda9167141039f193aa7d8e0985b","141090c1d7654d9eb4922f3154df2098"]}},"source":["classes_map = ProcessTinyImageNet.all_class_map()\n","obj = ProcessTinyImageNet(classes_map)\n","#targets = torch.as_tensor(obj.target)\n","dataset = list(zip(obj.image, obj.target))\n","random.shuffle(dataset)\n","trainlen = int(len(dataset)*0.7)\n","classes = list(classes_map.values())\n","print(len(dataset), trainlen, len(classes))"],"execution_count":9,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2399de4a4bec4d4bacb122b6f0ff3610","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Loading train folder...', max=200.0, style=ProgressStyle(…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"881cb1fd1d5444c2812fd888e712dfd0","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Loading val folder....', max=1.0, style…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","110000 77000 200\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kdKRRQWX44IY","executionInfo":{"status":"ok","timestamp":1603033600486,"user_tz":-330,"elapsed":1014,"user":{"displayName":"Prasad Shripathi","photoUrl":"","userId":"01122758420514335828"}}},"source":["import torch\n","from skimage import io\n","import numpy as np\n","from torch.utils.data import Dataset\n","from sklearn import preprocessing\n","from PIL import Image\n","\n","class TinyImagenetDataset(Dataset):\n","    \"\"\"Face Landmarks dataset.\"\"\"\n","\n","    def __init__(self, data, transform=None):\n","        \"\"\"\n","        Args:\n","            data (string): zipped images and labels.\n","        \"\"\"\n","        self.transform = transform\n","        self.images, self.labels = zip(*data)\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","        image = io.imread(self.images[idx], as_gray=False, pilmode=\"RGB\")\n","        # image = Image.open(self.images)\n","        # npimg = np.asarray(image)\n","        # if (len(npimg.shape) == 2):\n","        #     npimg = np.repeat(npimg[:, :, np.newaxis], 3, axis=2)\n","        le = preprocessing.LabelEncoder()\n","        targets = le.fit_transform(self.labels)\n","        self.labels = torch.as_tensor(targets)\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, self.labels[idx] #label"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ed2ntG-nACxR","executionInfo":{"status":"ok","timestamp":1603033604459,"user_tz":-330,"elapsed":1168,"user":{"displayName":"Prasad Shripathi","photoUrl":"","userId":"01122758420514335828"}}},"source":["import albumentations as A\n","class Albumentation:\n","  \"\"\"\n","  Helper class to create test and train transforms using Albumentations\n","  \"\"\"\n","  def __init__(self, transforms=[]):\n","    self.transforms = A.Compose(transforms)\n","\n","  def __call__(self, img):\n","    img = np.array(img)\n","    return self.transforms(image=img)['image']"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"0CfsUOqzAJGf","executionInfo":{"status":"ok","timestamp":1603033606850,"user_tz":-330,"elapsed":1075,"user":{"displayName":"Prasad Shripathi","photoUrl":"","userId":"01122758420514335828"}}},"source":["patch_size = 32\n","mean = (0.48043839, 0.44820218, 0.39760034)\n","std = (0.27698959, 0.26908774, 0.28216029)\n","custom_transforms = [\n","       A.PadIfNeeded(min_height=70, min_width=70),\n","       A.RandomCrop(64, 64, p=1.0),\n","       A.HorizontalFlip(p=0.25),\n","       A.Rotate(limit=15, p=0.25),\n","       A.Normalize(mean=mean, std=std),\n","       A.Cutout(num_holes=4, max_h_size=16, max_w_size=16),\n","    ]\n","\n","train_transforms = Albumentation(custom_transforms)\n","test_transforms = Albumentation(\n","    [\n","     A.Normalize(mean=mean, std=std),\n","     ])"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"x_wsTklZASBw","executionInfo":{"status":"ok","timestamp":1603033609786,"user_tz":-330,"elapsed":997,"user":{"displayName":"Prasad Shripathi","photoUrl":"","userId":"01122758420514335828"}}},"source":["\n","# train_dataset = TorchVisionDataSet(train, transform=train_transforms)\n","# test_dataset = TorchVisionDataSet(test, transform=test_transforms)\n","train_dataset = TinyImagenetDataset(dataset[:trainlen], transform=train_transforms)\n","test_dataset = TinyImagenetDataset(dataset[trainlen:], transform=test_transforms)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"dWLhL6E4BFfx","executionInfo":{"status":"ok","timestamp":1603037059563,"user_tz":-330,"elapsed":1206,"user":{"displayName":"Prasad Shripathi","photoUrl":"","userId":"01122758420514335828"}}},"source":["\n","class DataLoader(object):\n","\n","    def __init__(self, train, test, batch_size=128):\n","        self.train_loader, self.test_loader = self.data_loader(train, test, batch_size)\n","\n","    def data_loader(self, train, test, batch_size=128,  num_workers=4):\n","        SEED = 1\n","        # CUDA?\n","        cuda = torch.cuda.is_available()\n","        print(\"CUDA Available?\", cuda)\n","\n","        # For reproducibility\n","        torch.manual_seed(SEED)\n","\n","        if cuda:\n","            torch.cuda.manual_seed(SEED)\n","\n","        # dataloader arguments - something you'll fetch these from cmdprmt\n","        dataloader_args = dict(shuffle=True, batch_size=batch_size, num_workers=num_workers, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n","\n","        # train dataloader\n","        train_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n","\n","        # test dataloader\n","        test_loader = torch.utils.data.DataLoader(test, **dataloader_args)\n","        return train_loader, test_loader"],"execution_count":44,"outputs":[]},{"cell_type":"code","metadata":{"id":"dyG1WZj7dNF4","executionInfo":{"status":"ok","timestamp":1603037096084,"user_tz":-330,"elapsed":1089,"user":{"displayName":"Prasad Shripathi","photoUrl":"","userId":"01122758420514335828"}},"outputId":"0228b350-a128-47cb-9db2-32c028d64ffc","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["\n","data_loader_obj = DataLoader(train_dataset, test_dataset, batch_size=256)\n","train_loader = data_loader_obj.train_loader\n","test_loader = data_loader_obj.test_loader"],"execution_count":45,"outputs":[{"output_type":"stream","text":["CUDA Available? True\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"26hGonThdQe2"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"llcQzYYNZH_m","executionInfo":{"status":"error","timestamp":1603036788034,"user_tz":-330,"elapsed":1192,"user":{"displayName":"Prasad Shripathi","photoUrl":"","userId":"01122758420514335828"}},"outputId":"06438404-8783-443f-a650-ac0b7f8dd3c0","colab":{"base_uri":"https://localhost:8080/","height":166}},"source":["train_loader.shape"],"execution_count":43,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-43-845fa6b79289>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'shape'"]}]},{"cell_type":"code","metadata":{"id":"EVcuH1FbGuMT","executionInfo":{"status":"ok","timestamp":1603033622178,"user_tz":-330,"elapsed":5672,"user":{"displayName":"Prasad Shripathi","photoUrl":"","userId":"01122758420514335828"}},"outputId":"fd20b6eb-ab02-49f2-b69a-b6c8fa38ed90","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from torchsummary import summary\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","model = Resnet.ResNet18().to(device)\n","summary(model, input_size=(3, 64, 64))"],"execution_count":15,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 64, 64, 64]           1,728\n","       BatchNorm2d-2           [-1, 64, 64, 64]             128\n","            Conv2d-3           [-1, 64, 64, 64]          36,864\n","       BatchNorm2d-4           [-1, 64, 64, 64]             128\n","            Conv2d-5           [-1, 64, 64, 64]          36,864\n","       BatchNorm2d-6           [-1, 64, 64, 64]             128\n","        BasicBlock-7           [-1, 64, 64, 64]               0\n","            Conv2d-8           [-1, 64, 64, 64]          36,864\n","       BatchNorm2d-9           [-1, 64, 64, 64]             128\n","           Conv2d-10           [-1, 64, 64, 64]          36,864\n","      BatchNorm2d-11           [-1, 64, 64, 64]             128\n","       BasicBlock-12           [-1, 64, 64, 64]               0\n","           Conv2d-13          [-1, 128, 32, 32]          73,728\n","      BatchNorm2d-14          [-1, 128, 32, 32]             256\n","           Conv2d-15          [-1, 128, 32, 32]         147,456\n","      BatchNorm2d-16          [-1, 128, 32, 32]             256\n","           Conv2d-17          [-1, 128, 32, 32]           8,192\n","      BatchNorm2d-18          [-1, 128, 32, 32]             256\n","       BasicBlock-19          [-1, 128, 32, 32]               0\n","           Conv2d-20          [-1, 128, 32, 32]         147,456\n","      BatchNorm2d-21          [-1, 128, 32, 32]             256\n","           Conv2d-22          [-1, 128, 32, 32]         147,456\n","      BatchNorm2d-23          [-1, 128, 32, 32]             256\n","       BasicBlock-24          [-1, 128, 32, 32]               0\n","           Conv2d-25          [-1, 256, 16, 16]         294,912\n","      BatchNorm2d-26          [-1, 256, 16, 16]             512\n","           Conv2d-27          [-1, 256, 16, 16]         589,824\n","      BatchNorm2d-28          [-1, 256, 16, 16]             512\n","           Conv2d-29          [-1, 256, 16, 16]          32,768\n","      BatchNorm2d-30          [-1, 256, 16, 16]             512\n","       BasicBlock-31          [-1, 256, 16, 16]               0\n","           Conv2d-32          [-1, 256, 16, 16]         589,824\n","      BatchNorm2d-33          [-1, 256, 16, 16]             512\n","           Conv2d-34          [-1, 256, 16, 16]         589,824\n","      BatchNorm2d-35          [-1, 256, 16, 16]             512\n","       BasicBlock-36          [-1, 256, 16, 16]               0\n","           Conv2d-37            [-1, 512, 8, 8]       1,179,648\n","      BatchNorm2d-38            [-1, 512, 8, 8]           1,024\n","           Conv2d-39            [-1, 512, 8, 8]       2,359,296\n","      BatchNorm2d-40            [-1, 512, 8, 8]           1,024\n","           Conv2d-41            [-1, 512, 8, 8]         131,072\n","      BatchNorm2d-42            [-1, 512, 8, 8]           1,024\n","       BasicBlock-43            [-1, 512, 8, 8]               0\n","           Conv2d-44            [-1, 512, 8, 8]       2,359,296\n","      BatchNorm2d-45            [-1, 512, 8, 8]           1,024\n","           Conv2d-46            [-1, 512, 8, 8]       2,359,296\n","      BatchNorm2d-47            [-1, 512, 8, 8]           1,024\n","       BasicBlock-48            [-1, 512, 8, 8]               0\n","           Linear-49                  [-1, 200]         102,600\n","================================================================\n","Total params: 11,271,432\n","Trainable params: 11,271,432\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.05\n","Forward/backward pass size (MB): 45.00\n","Params size (MB): 43.00\n","Estimated Total Size (MB): 88.05\n","----------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eMUyy6frRLdz","executionInfo":{"status":"ok","timestamp":1603034609083,"user_tz":-330,"elapsed":1141,"user":{"displayName":"Prasad Shripathi","photoUrl":"","userId":"01122758420514335828"}}},"source":["\n","#initializers \n","dropout_value = 0.05\n","num_splits=2\n","start_epoch=0\n","EPOCHS = 24\n","EPOCHS1 = 50\n","input_ch=3\n","best_acc = 60\n","\n","metric_values = {}\n","models={}\n","\n","batch_size_GPU=256\n","batch_size_CPU=16\n","\n","model_save_path = my_path+\"/best_model.pt\"\n"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"b6_w_ceQB4oB","executionInfo":{"status":"ok","timestamp":1603033890806,"user_tz":-330,"elapsed":976,"user":{"displayName":"Prasad Shripathi","photoUrl":"","userId":"01122758420514335828"}}},"source":["from torch.optim.lr_scheduler import StepLR\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","scheduler = StepLR(optimizer, step_size=6, gamma=0.05)"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"wpWlVSHDrwBm"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JHsfji5hQ_vJ","executionInfo":{"status":"error","timestamp":1603040411782,"user_tz":-330,"elapsed":14822,"user":{"displayName":"Prasad Shripathi","photoUrl":"","userId":"01122758420514335828"}},"outputId":"0207c946-0732-408a-cd24-f3ec755caa51","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["exp_name = 'Best Learning Rate'\n","\n","trainer = Test_Train_OCP.Trainer(model,device,train_loader,test_loader,optimizer,start_epoch,EPOCHS,scheduler,model_save_path,best_acc)\n","train_metric, test_metric,learning_rate = trainer.train(l1_lambda=0, l2_lambda=0)\n","\n","# save the metrics in dictionary\n","metric_values[exp_name] = (train_metric, test_metric)"],"execution_count":48,"outputs":[{"output_type":"stream","text":["\n","\n","\n","\n","\n","\n","\n","\n","  0%|          | 0/301 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["EPOCH: 0\n","learning rate  0.01\n"],"name":"stdout"},{"output_type":"stream","text":["Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f0d5e5bb278>>\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f0d5e5bb278>>\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f0d5e5bb278>>\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f0d5e5bb278>>\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1101, in __del__\n","Traceback (most recent call last):\n","    self._shutdown_workers()\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1101, in __del__\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1075, in _shutdown_workers\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1101, in __del__\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1101, in __del__\n","    self._shutdown_workers()\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","    self._shutdown_workers()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1075, in _shutdown_workers\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1075, in _shutdown_workers\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1075, in _shutdown_workers\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","AssertionError: can only join a child process\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f0d5e3abf28>>\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f0d5e3abf28>>\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f0d5e3abf28>>\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1101, in __del__\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1101, in __del__\n","    self._shutdown_workers()\n","    self._shutdown_workers()\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f0d5e3abf28>>\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1101, in __del__\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1075, in _shutdown_workers\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1075, in _shutdown_workers\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1075, in _shutdown_workers\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1101, in __del__\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","    self._shutdown_workers()\n","AssertionError: can only join a child process\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1075, in _shutdown_workers\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","AssertionError: can only join a child process\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f0d5e355710>>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1101, in __del__\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f0d5e355710>>\n","    self._shutdown_workers()\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f0d5e355710>>\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1075, in _shutdown_workers\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1101, in __del__\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f0d5e355710>>\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1101, in __del__\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","Traceback (most recent call last):\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1101, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1075, in _shutdown_workers\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1075, in _shutdown_workers\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1075, in _shutdown_workers\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","AssertionError: can only join a child process\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f0d876ecd68>>\n","Traceback (most recent call last):\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f0d876ecd68>>\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1101, in __del__\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f0d876ecd68>>\n","Traceback (most recent call last):\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1101, in __del__\n","    self._shutdown_workers()\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1075, in _shutdown_workers\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1075, in _shutdown_workers\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1101, in __del__\n","    self._shutdown_workers()\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f0d876ecd68>>\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1075, in _shutdown_workers\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","Traceback (most recent call last):\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1101, in __del__\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","    self._shutdown_workers()\n","AssertionError: can only join a child process\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1075, in _shutdown_workers\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f0d8758c550>>\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f0d8758c550>>\n","Traceback (most recent call last):\n","AssertionError: can only join a child process\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1101, in __del__\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1101, in __del__\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f0d8758c550>>\n","    self._shutdown_workers()\n","    self._shutdown_workers()\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1075, in _shutdown_workers\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1101, in __del__\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1075, in _shutdown_workers\n","    self._shutdown_workers()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f0d8758c550>>\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1075, in _shutdown_workers\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","Traceback (most recent call last):\n","AssertionError: can only join a child process\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1101, in __del__\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    self._shutdown_workers()\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f0d87394eb8>>\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1075, in _shutdown_workers\n","Traceback (most recent call last):\n","AssertionError: can only join a child process\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1101, in __del__\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1075, in _shutdown_workers\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f0d87394eb8>>\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f0d87394eb8>>\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1101, in __del__\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1101, in __del__\n","    self._shutdown_workers()\n","    self._shutdown_workers()\n","AssertionError: can only join a child process\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1075, in _shutdown_workers\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1075, in _shutdown_workers\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n","Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f0d87394eb8>>\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1101, in __del__\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","    self._shutdown_workers()\n","AssertionError: can only join a child process\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1075, in _shutdown_workers\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 122, in join\n","    assert self._parent_pid == os.getpid(), 'can only join a child process'\n","AssertionError: can only join a child process\n"],"name":"stderr"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-48-8d9ec01edc7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTest_Train_OCP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_save_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_metric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1_lambda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_lambda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# save the metrics in dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/Colab Notebooks/EVA 5/12.Yolo+Tinyimagenet/Test_Train_OCP.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, l1_lambda, l2_lambda)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;31m# trainer = Trainer(model,device,train_loader,test_loader,optimizer,epoch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mtrain_ac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_los\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mtest_ac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_los\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_mod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/Colab Notebooks/EVA 5/12.Yolo+Tinyimagenet/Test_Train_OCP.py\u001b[0m in \u001b[0;36mtrain_mod\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m# Predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;31m# Calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/Colab Notebooks/EVA 5/12.Yolo+Tinyimagenet/Resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    414\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    415\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 416\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 3, 3], expected input[256, 64, 64, 3] to have 3 channels, but got 64 channels instead"]}]},{"cell_type":"code","metadata":{"id":"teZrUKhfTh_-"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JOC45-uoS9NG"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RIxmekp0RFRj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NPE7M3i-Qobr"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dfTt7mvT-3JG"},"source":["##Build Resnet Model"]},{"cell_type":"code","metadata":{"id":"S9Mu_RNRBS-Z","outputId":"60eded54-6f07-418e-994d-4a81a4271e3c","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import six\n","from keras.models import Model\n","from keras.layers import ( Input, Activation, Dense, Flatten )\n","from keras.layers.convolutional import ( Conv2D, MaxPooling2D, AveragePooling2D )\n","from keras.layers.merge import add\n","from keras.layers.normalization import BatchNormalization\n","from keras.regularizers import l2\n","from keras import backend as K\n","\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.utils import np_utils\n","from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping\n","\n","import numpy as np\n","\n","def _bn_relu(input):\n","    \"\"\"Helper to build a BN -> relu block\"\"\"\n","    norm = BatchNormalization(axis=CHANNEL_AXIS)(input)\n","    return Activation(\"relu\")(norm)\n","\n","def _conv_bn_relu(**conv_params):\n","    \"\"\"Helper to build a conv -> BN -> relu block\"\"\"\n","    filters = conv_params[\"filters\"]\n","    kernel_size = conv_params[\"kernel_size\"]\n","    strides = conv_params.setdefault(\"strides\", (1, 1))\n","    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n","    padding = conv_params.setdefault(\"padding\", \"same\")\n","    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n","\n","    def f(input):\n","        conv = Conv2D(filters=filters, kernel_size=kernel_size,\n","                      strides=strides, padding=padding,\n","                      kernel_initializer=kernel_initializer,\n","                      kernel_regularizer=kernel_regularizer)(input)\n","        return _bn_relu(conv)\n","\n","    return f\n","\n","def _bn_relu_conv(**conv_params):\n","    \"\"\"Helper to build a BN -> relu -> conv block.\n","    This is an improved scheme proposed in http://arxiv.org/pdf/1603.05027v2.pdf \"\"\"\n","    filters = conv_params[\"filters\"]\n","    kernel_size = conv_params[\"kernel_size\"]\n","    strides = conv_params.setdefault(\"strides\", (1, 1))\n","    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n","    padding = conv_params.setdefault(\"padding\", \"same\")\n","    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n","\n","    def f(input):\n","        activation = _bn_relu(input)\n","        return Conv2D(filters=filters, kernel_size=kernel_size,\n","                      strides=strides, padding=padding,\n","                      kernel_initializer=kernel_initializer,\n","                      kernel_regularizer=kernel_regularizer)(activation)\n","\n","    return f\n","\n","def _shortcut(input, residual):\n","    \"\"\"Adds a shortcut between input and residual block and merges them with \"sum\" \"\"\"\n","    # Expand channels of shortcut to match residual.\n","    # Stride appropriately to match residual (width, height)\n","    # Should be int if network architecture is correctly configured.\n","    input_shape = K.int_shape(input)\n","    residual_shape = K.int_shape(residual)\n","    stride_width = int(round(input_shape[ROW_AXIS] / residual_shape[ROW_AXIS]))\n","    stride_height = int(round(input_shape[COL_AXIS] / residual_shape[COL_AXIS]))\n","    equal_channels = input_shape[CHANNEL_AXIS] == residual_shape[CHANNEL_AXIS]\n","\n","    shortcut = input\n","    # 1 X 1 conv if shape is different. Else identity.\n","    if stride_width > 1 or stride_height > 1 or not equal_channels:\n","        shortcut = Conv2D(filters=residual_shape[CHANNEL_AXIS],\n","                          kernel_size=(1, 1),\n","                          strides=(stride_width, stride_height),\n","                          padding=\"valid\",\n","                          kernel_initializer=\"he_normal\",\n","                          kernel_regularizer=l2(0.0001))(input)\n","\n","    return add([shortcut, residual])\n","\n","\n","def _residual_block(block_function, filters, repetitions, is_first_layer=False):\n","    \"\"\"Builds a residual block with repeating bottleneck blocks.\"\"\"\n","    def f(input):\n","        for i in range(repetitions):\n","            init_strides = (1, 1)\n","            if i == 0 and not is_first_layer:\n","                init_strides = (2, 2)\n","            input = block_function(filters=filters, init_strides=init_strides,\n","                                   is_first_block_of_first_layer=(is_first_layer and i == 0))(input)\n","        return input\n","    return f\n","\n","\n","def basic_block(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n","    \"\"\"Basic 3 X 3 convolution blocks for use on resnets with layers <= 34.\n","    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf \"\"\"\n","    def f(input):\n","\n","        if is_first_block_of_first_layer:\n","            # don't repeat bn->relu since we just did bn->relu->maxpool\n","            conv1 = Conv2D(filters=filters, kernel_size=(3, 3),\n","                           strides=init_strides,\n","                           padding=\"same\",\n","                           kernel_initializer=\"he_normal\",\n","                           kernel_regularizer=l2(1e-4))(input)\n","        else:\n","            conv1 = _bn_relu_conv(filters=filters, kernel_size=(3, 3),\n","                                  strides=init_strides)(input)\n","\n","        residual = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv1)\n","        return _shortcut(input, residual)\n","\n","    return f\n","\n","\n","def bottleneck(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n","    \"\"\"Bottleneck architecture for > 34 layer resnet.\n","    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n","    Returns:\n","        A final conv layer of filters * 4 \"\"\"\n","    def f(input):\n","\n","        if is_first_block_of_first_layer:\n","            # don't repeat bn->relu since we just did bn->relu->maxpool\n","            conv_1_1 = Conv2D(filters=filters, kernel_size=(1, 1),\n","                              strides=init_strides,\n","                              padding=\"same\",\n","                              kernel_initializer=\"he_normal\",\n","                              kernel_regularizer=l2(1e-4))(input)\n","        else:\n","            conv_1_1 = _bn_relu_conv(filters=filters, kernel_size=(1, 1),\n","                                     strides=init_strides)(input)\n","\n","        conv_3_3 = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv_1_1)\n","        residual = _bn_relu_conv(filters=filters * 4, kernel_size=(1,1))(conv_3_3)\n","        return _shortcut(input, residual)\n","\n","    return f\n","\n","\n","def _handle_dim_ordering():\n","    global ROW_AXIS\n","    global COL_AXIS\n","    global CHANNEL_AXIS\n","    if K.image_dim_ordering() == 'tf':\n","        ROW_AXIS = 1\n","        COL_AXIS = 2\n","        CHANNEL_AXIS = 3\n","    else:\n","        CHANNEL_AXIS = 1\n","        ROW_AXIS = 2\n","        COL_AXIS = 3\n","\n","\n","def _get_block(identifier):\n","    if isinstance(identifier, six.string_types):\n","        res = globals().get(identifier)\n","        if not res:\n","            raise ValueError('Invalid {}'.format(identifier))\n","        return res\n","    return identifier\n","\n","\n","class ResnetBuilder(object):\n","    @staticmethod\n","    def build(input_shape, num_outputs, block_fn, repetitions):\n","        \"\"\"Builds a custom ResNet like architecture.\n","        Args:\n","            input_shape: The input shape in the form (nb_channels, nb_rows, nb_cols)\n","            num_outputs: The number of outputs at final softmax layer\n","            block_fn: The block function to use. This is either `basic_block` or `bottleneck`.\n","                The original paper used basic_block for layers < 50\n","            repetitions: Number of repetitions of various block units.\n","                At each block unit, the number of filters are doubled and the input size is halved\n","        Returns:\n","            The keras 'Model'.\n","        \"\"\"\n","        _handle_dim_ordering()\n","        if len(input_shape) != 3:\n","            raise Exception(\"Input shape should be a tuple (nb_channels, nb_rows, nb_cols)\")\n","\n","        # Permute dimension order if necessary\n","        if K.image_dim_ordering() == 'tf':\n","            input_shape = (input_shape[1], input_shape[2], input_shape[0])\n","\n","        # Load function from str if needed.\n","        block_fn = _get_block(block_fn)\n","\n","        input = Input(shape=input_shape)\n","        conv1 = _conv_bn_relu(filters=64, kernel_size=(7, 7), strides=(2, 2))(input)\n","        pool1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(conv1)\n","\n","        block = pool1\n","        filters = 64\n","        for i, r in enumerate(repetitions):\n","            block = _residual_block(block_fn, filters=filters, repetitions=r, is_first_layer=(i == 0))(block)\n","            filters *= 2\n","\n","        # Last activation\n","        block = _bn_relu(block)\n","\n","        # Classifier block\n","        block_shape = K.int_shape(block)\n","        pool2 = AveragePooling2D(pool_size=(block_shape[ROW_AXIS], block_shape[COL_AXIS]),\n","                                 strides=(1, 1))(block)\n"," \n","        out = Conv2D(filters=num_outputs, kernel_size=(1, 1),\n","                              strides=(1,1),\n","                              kernel_initializer=\"he_normal\",\n","                              kernel_regularizer=l2(1e-4),\n","                              activation=\"softmax\")(pool2)\n","        flatten1 = Flatten()(out)\n","        model = Model(inputs=input, outputs=flatten1)\n","        return model\n","\n","    @staticmethod\n","    def build_resnet_50(input_shape, num_outputs):\n","        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 6, 3])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"5vJNK1FwDszD","outputId":"0b898480-0ec6-4c68-cf97-91ced9809496","colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n","early_stopper = EarlyStopping(min_delta=0.001, patience=10)\n","csv_logger = CSVLogger('resnet50_tiny_ImageNet.csv')\n","\n","batch_size = 500\n","nb_classes = 200\n","nb_epoch = 10\n","\n","# input image dimensions\n","img_rows, img_cols = 64, 64\n","# The images are RGB\n","img_channels = 3\n","\n","# The data, shuffled and split between train and test sets:\n","X_train = train_data\n","Y_train = train_labels\n","X_test = test_data\n","Y_test = test_labels\n","\n","X_train = X_train.astype('float32')\n","X_test = X_test.astype('float32')\n","\n","# subtract mean and normalize\n","mean_image = np.mean(X_train, axis=0)\n","X_train -= mean_image\n","X_test -= mean_image\n","X_train /= 128.\n","X_test /= 128.\n","\n","model = ResnetBuilder.build_resnet_50((img_channels, img_rows, img_cols), nb_classes)\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DAgstQeTDy_S","outputId":"adac8f54-9439-4828-8f7d-9a3c9fbe53ce","colab":{"base_uri":"https://localhost:8080/","height":6256}},"source":["model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, 64, 64, 3)    0                                            \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 32, 32, 64)   9472        input_1[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 32, 32, 64)   256         conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 32, 32, 64)   0           batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 64)   0           activation_1[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 16, 16, 64)   4160        max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 16, 16, 64)   256         conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 16, 16, 64)   0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 16, 16, 64)   36928       activation_2[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 16, 16, 64)   256         conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 16, 16, 64)   0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_5 (Conv2D)               (None, 16, 16, 256)  16640       max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 16, 16, 256)  16640       activation_3[0][0]               \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 16, 16, 256)  0           conv2d_5[0][0]                   \n","                                                                 conv2d_4[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 16, 16, 256)  1024        add_1[0][0]                      \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 16, 16, 256)  0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_6 (Conv2D)               (None, 16, 16, 64)   16448       activation_4[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 16, 16, 64)   256         conv2d_6[0][0]                   \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 16, 16, 64)   0           batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_7 (Conv2D)               (None, 16, 16, 64)   36928       activation_5[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 16, 16, 64)   256         conv2d_7[0][0]                   \n","__________________________________________________________________________________________________\n","activation_6 (Activation)       (None, 16, 16, 64)   0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_8 (Conv2D)               (None, 16, 16, 256)  16640       activation_6[0][0]               \n","__________________________________________________________________________________________________\n","add_2 (Add)                     (None, 16, 16, 256)  0           add_1[0][0]                      \n","                                                                 conv2d_8[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 16, 16, 256)  1024        add_2[0][0]                      \n","__________________________________________________________________________________________________\n","activation_7 (Activation)       (None, 16, 16, 256)  0           batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_9 (Conv2D)               (None, 16, 16, 64)   16448       activation_7[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_8 (BatchNor (None, 16, 16, 64)   256         conv2d_9[0][0]                   \n","__________________________________________________________________________________________________\n","activation_8 (Activation)       (None, 16, 16, 64)   0           batch_normalization_8[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_10 (Conv2D)              (None, 16, 16, 64)   36928       activation_8[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_9 (BatchNor (None, 16, 16, 64)   256         conv2d_10[0][0]                  \n","__________________________________________________________________________________________________\n","activation_9 (Activation)       (None, 16, 16, 64)   0           batch_normalization_9[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_11 (Conv2D)              (None, 16, 16, 256)  16640       activation_9[0][0]               \n","__________________________________________________________________________________________________\n","add_3 (Add)                     (None, 16, 16, 256)  0           add_2[0][0]                      \n","                                                                 conv2d_11[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_10 (BatchNo (None, 16, 16, 256)  1024        add_3[0][0]                      \n","__________________________________________________________________________________________________\n","activation_10 (Activation)      (None, 16, 16, 256)  0           batch_normalization_10[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_12 (Conv2D)              (None, 8, 8, 128)    32896       activation_10[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_11 (BatchNo (None, 8, 8, 128)    512         conv2d_12[0][0]                  \n","__________________________________________________________________________________________________\n","activation_11 (Activation)      (None, 8, 8, 128)    0           batch_normalization_11[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_13 (Conv2D)              (None, 8, 8, 128)    147584      activation_11[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_12 (BatchNo (None, 8, 8, 128)    512         conv2d_13[0][0]                  \n","__________________________________________________________________________________________________\n","activation_12 (Activation)      (None, 8, 8, 128)    0           batch_normalization_12[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_15 (Conv2D)              (None, 8, 8, 512)    131584      add_3[0][0]                      \n","__________________________________________________________________________________________________\n","conv2d_14 (Conv2D)              (None, 8, 8, 512)    66048       activation_12[0][0]              \n","__________________________________________________________________________________________________\n","add_4 (Add)                     (None, 8, 8, 512)    0           conv2d_15[0][0]                  \n","                                                                 conv2d_14[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_13 (BatchNo (None, 8, 8, 512)    2048        add_4[0][0]                      \n","__________________________________________________________________________________________________\n","activation_13 (Activation)      (None, 8, 8, 512)    0           batch_normalization_13[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_16 (Conv2D)              (None, 8, 8, 128)    65664       activation_13[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_14 (BatchNo (None, 8, 8, 128)    512         conv2d_16[0][0]                  \n","__________________________________________________________________________________________________\n","activation_14 (Activation)      (None, 8, 8, 128)    0           batch_normalization_14[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_17 (Conv2D)              (None, 8, 8, 128)    147584      activation_14[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_15 (BatchNo (None, 8, 8, 128)    512         conv2d_17[0][0]                  \n","__________________________________________________________________________________________________\n","activation_15 (Activation)      (None, 8, 8, 128)    0           batch_normalization_15[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_18 (Conv2D)              (None, 8, 8, 512)    66048       activation_15[0][0]              \n","__________________________________________________________________________________________________\n","add_5 (Add)                     (None, 8, 8, 512)    0           add_4[0][0]                      \n","                                                                 conv2d_18[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_16 (BatchNo (None, 8, 8, 512)    2048        add_5[0][0]                      \n","__________________________________________________________________________________________________\n","activation_16 (Activation)      (None, 8, 8, 512)    0           batch_normalization_16[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_19 (Conv2D)              (None, 8, 8, 128)    65664       activation_16[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_17 (BatchNo (None, 8, 8, 128)    512         conv2d_19[0][0]                  \n","__________________________________________________________________________________________________\n","activation_17 (Activation)      (None, 8, 8, 128)    0           batch_normalization_17[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_20 (Conv2D)              (None, 8, 8, 128)    147584      activation_17[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_18 (BatchNo (None, 8, 8, 128)    512         conv2d_20[0][0]                  \n","__________________________________________________________________________________________________\n","activation_18 (Activation)      (None, 8, 8, 128)    0           batch_normalization_18[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_21 (Conv2D)              (None, 8, 8, 512)    66048       activation_18[0][0]              \n","__________________________________________________________________________________________________\n","add_6 (Add)                     (None, 8, 8, 512)    0           add_5[0][0]                      \n","                                                                 conv2d_21[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_19 (BatchNo (None, 8, 8, 512)    2048        add_6[0][0]                      \n","__________________________________________________________________________________________________\n","activation_19 (Activation)      (None, 8, 8, 512)    0           batch_normalization_19[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_22 (Conv2D)              (None, 8, 8, 128)    65664       activation_19[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_20 (BatchNo (None, 8, 8, 128)    512         conv2d_22[0][0]                  \n","__________________________________________________________________________________________________\n","activation_20 (Activation)      (None, 8, 8, 128)    0           batch_normalization_20[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_23 (Conv2D)              (None, 8, 8, 128)    147584      activation_20[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_21 (BatchNo (None, 8, 8, 128)    512         conv2d_23[0][0]                  \n","__________________________________________________________________________________________________\n","activation_21 (Activation)      (None, 8, 8, 128)    0           batch_normalization_21[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_24 (Conv2D)              (None, 8, 8, 512)    66048       activation_21[0][0]              \n","__________________________________________________________________________________________________\n","add_7 (Add)                     (None, 8, 8, 512)    0           add_6[0][0]                      \n","                                                                 conv2d_24[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_22 (BatchNo (None, 8, 8, 512)    2048        add_7[0][0]                      \n","__________________________________________________________________________________________________\n","activation_22 (Activation)      (None, 8, 8, 512)    0           batch_normalization_22[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_25 (Conv2D)              (None, 4, 4, 256)    131328      activation_22[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_23 (BatchNo (None, 4, 4, 256)    1024        conv2d_25[0][0]                  \n","__________________________________________________________________________________________________\n","activation_23 (Activation)      (None, 4, 4, 256)    0           batch_normalization_23[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_26 (Conv2D)              (None, 4, 4, 256)    590080      activation_23[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_24 (BatchNo (None, 4, 4, 256)    1024        conv2d_26[0][0]                  \n","__________________________________________________________________________________________________\n","activation_24 (Activation)      (None, 4, 4, 256)    0           batch_normalization_24[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_28 (Conv2D)              (None, 4, 4, 1024)   525312      add_7[0][0]                      \n","__________________________________________________________________________________________________\n","conv2d_27 (Conv2D)              (None, 4, 4, 1024)   263168      activation_24[0][0]              \n","__________________________________________________________________________________________________\n","add_8 (Add)                     (None, 4, 4, 1024)   0           conv2d_28[0][0]                  \n","                                                                 conv2d_27[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_25 (BatchNo (None, 4, 4, 1024)   4096        add_8[0][0]                      \n","__________________________________________________________________________________________________\n","activation_25 (Activation)      (None, 4, 4, 1024)   0           batch_normalization_25[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_29 (Conv2D)              (None, 4, 4, 256)    262400      activation_25[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_26 (BatchNo (None, 4, 4, 256)    1024        conv2d_29[0][0]                  \n","__________________________________________________________________________________________________\n","activation_26 (Activation)      (None, 4, 4, 256)    0           batch_normalization_26[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_30 (Conv2D)              (None, 4, 4, 256)    590080      activation_26[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_27 (BatchNo (None, 4, 4, 256)    1024        conv2d_30[0][0]                  \n","__________________________________________________________________________________________________\n","activation_27 (Activation)      (None, 4, 4, 256)    0           batch_normalization_27[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_31 (Conv2D)              (None, 4, 4, 1024)   263168      activation_27[0][0]              \n","__________________________________________________________________________________________________\n","add_9 (Add)                     (None, 4, 4, 1024)   0           add_8[0][0]                      \n","                                                                 conv2d_31[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_28 (BatchNo (None, 4, 4, 1024)   4096        add_9[0][0]                      \n","__________________________________________________________________________________________________\n","activation_28 (Activation)      (None, 4, 4, 1024)   0           batch_normalization_28[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_32 (Conv2D)              (None, 4, 4, 256)    262400      activation_28[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_29 (BatchNo (None, 4, 4, 256)    1024        conv2d_32[0][0]                  \n","__________________________________________________________________________________________________\n","activation_29 (Activation)      (None, 4, 4, 256)    0           batch_normalization_29[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_33 (Conv2D)              (None, 4, 4, 256)    590080      activation_29[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_30 (BatchNo (None, 4, 4, 256)    1024        conv2d_33[0][0]                  \n","__________________________________________________________________________________________________\n","activation_30 (Activation)      (None, 4, 4, 256)    0           batch_normalization_30[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_34 (Conv2D)              (None, 4, 4, 1024)   263168      activation_30[0][0]              \n","__________________________________________________________________________________________________\n","add_10 (Add)                    (None, 4, 4, 1024)   0           add_9[0][0]                      \n","                                                                 conv2d_34[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_31 (BatchNo (None, 4, 4, 1024)   4096        add_10[0][0]                     \n","__________________________________________________________________________________________________\n","activation_31 (Activation)      (None, 4, 4, 1024)   0           batch_normalization_31[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_35 (Conv2D)              (None, 4, 4, 256)    262400      activation_31[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_32 (BatchNo (None, 4, 4, 256)    1024        conv2d_35[0][0]                  \n","__________________________________________________________________________________________________\n","activation_32 (Activation)      (None, 4, 4, 256)    0           batch_normalization_32[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_36 (Conv2D)              (None, 4, 4, 256)    590080      activation_32[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_33 (BatchNo (None, 4, 4, 256)    1024        conv2d_36[0][0]                  \n","__________________________________________________________________________________________________\n","activation_33 (Activation)      (None, 4, 4, 256)    0           batch_normalization_33[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_37 (Conv2D)              (None, 4, 4, 1024)   263168      activation_33[0][0]              \n","__________________________________________________________________________________________________\n","add_11 (Add)                    (None, 4, 4, 1024)   0           add_10[0][0]                     \n","                                                                 conv2d_37[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_34 (BatchNo (None, 4, 4, 1024)   4096        add_11[0][0]                     \n","__________________________________________________________________________________________________\n","activation_34 (Activation)      (None, 4, 4, 1024)   0           batch_normalization_34[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_38 (Conv2D)              (None, 4, 4, 256)    262400      activation_34[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_35 (BatchNo (None, 4, 4, 256)    1024        conv2d_38[0][0]                  \n","__________________________________________________________________________________________________\n","activation_35 (Activation)      (None, 4, 4, 256)    0           batch_normalization_35[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_39 (Conv2D)              (None, 4, 4, 256)    590080      activation_35[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_36 (BatchNo (None, 4, 4, 256)    1024        conv2d_39[0][0]                  \n","__________________________________________________________________________________________________\n","activation_36 (Activation)      (None, 4, 4, 256)    0           batch_normalization_36[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_40 (Conv2D)              (None, 4, 4, 1024)   263168      activation_36[0][0]              \n","__________________________________________________________________________________________________\n","add_12 (Add)                    (None, 4, 4, 1024)   0           add_11[0][0]                     \n","                                                                 conv2d_40[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_37 (BatchNo (None, 4, 4, 1024)   4096        add_12[0][0]                     \n","__________________________________________________________________________________________________\n","activation_37 (Activation)      (None, 4, 4, 1024)   0           batch_normalization_37[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_41 (Conv2D)              (None, 4, 4, 256)    262400      activation_37[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_38 (BatchNo (None, 4, 4, 256)    1024        conv2d_41[0][0]                  \n","__________________________________________________________________________________________________\n","activation_38 (Activation)      (None, 4, 4, 256)    0           batch_normalization_38[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_42 (Conv2D)              (None, 4, 4, 256)    590080      activation_38[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_39 (BatchNo (None, 4, 4, 256)    1024        conv2d_42[0][0]                  \n","__________________________________________________________________________________________________\n","activation_39 (Activation)      (None, 4, 4, 256)    0           batch_normalization_39[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_43 (Conv2D)              (None, 4, 4, 1024)   263168      activation_39[0][0]              \n","__________________________________________________________________________________________________\n","add_13 (Add)                    (None, 4, 4, 1024)   0           add_12[0][0]                     \n","                                                                 conv2d_43[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_40 (BatchNo (None, 4, 4, 1024)   4096        add_13[0][0]                     \n","__________________________________________________________________________________________________\n","activation_40 (Activation)      (None, 4, 4, 1024)   0           batch_normalization_40[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_44 (Conv2D)              (None, 2, 2, 512)    524800      activation_40[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_41 (BatchNo (None, 2, 2, 512)    2048        conv2d_44[0][0]                  \n","__________________________________________________________________________________________________\n","activation_41 (Activation)      (None, 2, 2, 512)    0           batch_normalization_41[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_45 (Conv2D)              (None, 2, 2, 512)    2359808     activation_41[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_42 (BatchNo (None, 2, 2, 512)    2048        conv2d_45[0][0]                  \n","__________________________________________________________________________________________________\n","activation_42 (Activation)      (None, 2, 2, 512)    0           batch_normalization_42[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_47 (Conv2D)              (None, 2, 2, 2048)   2099200     add_13[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_46 (Conv2D)              (None, 2, 2, 2048)   1050624     activation_42[0][0]              \n","__________________________________________________________________________________________________\n","add_14 (Add)                    (None, 2, 2, 2048)   0           conv2d_47[0][0]                  \n","                                                                 conv2d_46[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_43 (BatchNo (None, 2, 2, 2048)   8192        add_14[0][0]                     \n","__________________________________________________________________________________________________\n","activation_43 (Activation)      (None, 2, 2, 2048)   0           batch_normalization_43[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_48 (Conv2D)              (None, 2, 2, 512)    1049088     activation_43[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_44 (BatchNo (None, 2, 2, 512)    2048        conv2d_48[0][0]                  \n","__________________________________________________________________________________________________\n","activation_44 (Activation)      (None, 2, 2, 512)    0           batch_normalization_44[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_49 (Conv2D)              (None, 2, 2, 512)    2359808     activation_44[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_45 (BatchNo (None, 2, 2, 512)    2048        conv2d_49[0][0]                  \n","__________________________________________________________________________________________________\n","activation_45 (Activation)      (None, 2, 2, 512)    0           batch_normalization_45[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_50 (Conv2D)              (None, 2, 2, 2048)   1050624     activation_45[0][0]              \n","__________________________________________________________________________________________________\n","add_15 (Add)                    (None, 2, 2, 2048)   0           add_14[0][0]                     \n","                                                                 conv2d_50[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_46 (BatchNo (None, 2, 2, 2048)   8192        add_15[0][0]                     \n","__________________________________________________________________________________________________\n","activation_46 (Activation)      (None, 2, 2, 2048)   0           batch_normalization_46[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_51 (Conv2D)              (None, 2, 2, 512)    1049088     activation_46[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_47 (BatchNo (None, 2, 2, 512)    2048        conv2d_51[0][0]                  \n","__________________________________________________________________________________________________\n","activation_47 (Activation)      (None, 2, 2, 512)    0           batch_normalization_47[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_52 (Conv2D)              (None, 2, 2, 512)    2359808     activation_47[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_48 (BatchNo (None, 2, 2, 512)    2048        conv2d_52[0][0]                  \n","__________________________________________________________________________________________________\n","activation_48 (Activation)      (None, 2, 2, 512)    0           batch_normalization_48[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_53 (Conv2D)              (None, 2, 2, 2048)   1050624     activation_48[0][0]              \n","__________________________________________________________________________________________________\n","add_16 (Add)                    (None, 2, 2, 2048)   0           add_15[0][0]                     \n","                                                                 conv2d_53[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_49 (BatchNo (None, 2, 2, 2048)   8192        add_16[0][0]                     \n","__________________________________________________________________________________________________\n","activation_49 (Activation)      (None, 2, 2, 2048)   0           batch_normalization_49[0][0]     \n","__________________________________________________________________________________________________\n","average_pooling2d_1 (AveragePoo (None, 1, 1, 2048)   0           activation_49[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_54 (Conv2D)              (None, 1, 1, 200)    409800      average_pooling2d_1[0][0]        \n","__________________________________________________________________________________________________\n","flatten_1 (Flatten)             (None, 200)          0           conv2d_54[0][0]                  \n","==================================================================================================\n","Total params: 23,982,152\n","Trainable params: 23,936,712\n","Non-trainable params: 45,440\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gDs8h4hEIfgC","outputId":"51eb6f0a-a372-45d1-f063-5e0c69a6a05c","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print('Using real-time data augmentation.')\n","# This will do preprocessing and realtime data augmentation:\n","datagen = ImageDataGenerator(\n","          featurewise_center=False,           # set input mean to 0 over the dataset\n","          samplewise_center=False,            # set each sample mean to 0\n","          featurewise_std_normalization=False,# divide inputs by std of the dataset\n","          samplewise_std_normalization=False, # divide each input by its std\n","          zca_whitening=False,                # apply ZCA whitening\n","          rotation_range=0,                   # randomly rotate images in the range (degrees, 0 to 180)\n","          width_shift_range=0.1,              # randomly shift images horizontally (fraction of total width)\n","          height_shift_range=0.1,             # randomly shift images vertically (fraction of total height)\n","          horizontal_flip=True,               # randomly flip images\n","          vertical_flip=False )               # randomly flip images\n","\n","# Compute quantities required for featurewise normalization\n","# (std, mean, and principal components if ZCA whitening is applied).\n","datagen.fit( X_train )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using real-time data augmentation.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"q5A5FkcTKe4e","outputId":"ddb4a518-f4fd-4c3c-b86f-b95e91a3763a","colab":{"base_uri":"https://localhost:8080/","height":530}},"source":["nb_epoch = 10 # epoch 1-10\n","# Fit the model on the batches generated by datagen.flow().\n","model.fit_generator( datagen.flow(X_train, Y_train, batch_size=batch_size),\n","                     steps_per_epoch=X_train.shape[0] // batch_size,\n","                     validation_data=(X_test, Y_test),\n","                     epochs=nb_epoch, verbose=1, max_q_size=100,\n","                     callbacks=[lr_reducer, early_stopper, csv_logger] )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., steps_per_epoch=200, validation_data=(array([[[..., epochs=10, verbose=1, callbacks=[<keras.ca..., max_queue_size=100)`\n","  import sys\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n","Epoch 1/10\n","200/200 [==============================] - 313s 2s/step - loss: 7.8622 - acc: 0.1091 - val_loss: 6.5234 - val_acc: 0.1111\n","Epoch 2/10\n","200/200 [==============================] - 284s 1s/step - loss: 5.4329 - acc: 0.2009 - val_loss: 5.2347 - val_acc: 0.1820\n","Epoch 3/10\n","200/200 [==============================] - 282s 1s/step - loss: 4.6477 - acc: 0.2485 - val_loss: 4.6894 - val_acc: 0.2170\n","Epoch 4/10\n","200/200 [==============================] - 282s 1s/step - loss: 4.2260 - acc: 0.2819 - val_loss: 4.4114 - val_acc: 0.2402\n","Epoch 5/10\n","200/200 [==============================] - 281s 1s/step - loss: 3.9444 - acc: 0.3107 - val_loss: 4.2279 - val_acc: 0.2517\n","Epoch 6/10\n","200/200 [==============================] - 281s 1s/step - loss: 3.7360 - acc: 0.3331 - val_loss: 4.0793 - val_acc: 0.2785\n","Epoch 7/10\n","200/200 [==============================] - 283s 1s/step - loss: 3.5748 - acc: 0.3553 - val_loss: 4.1140 - val_acc: 0.2661\n","Epoch 8/10\n","200/200 [==============================] - 284s 1s/step - loss: 3.4538 - acc: 0.3708 - val_loss: 3.8720 - val_acc: 0.3028\n","Epoch 9/10\n","200/200 [==============================] - 286s 1s/step - loss: 3.3490 - acc: 0.3863 - val_loss: 3.7867 - val_acc: 0.3144\n","Epoch 10/10\n","200/200 [==============================] - 285s 1s/step - loss: 3.2449 - acc: 0.4043 - val_loss: 3.6907 - val_acc: 0.3242\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f0338b92c18>"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"omUW3ElFEmXK"},"source":["epochs_passed = 10\n","model.save_weights( 'model_weights_Img_aug_after_%s_epoch.h5' % epochs_passed )\n","model.save( 'custom_resent50_model.h5' )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lHenqmhnxuwU"},"source":["from keras.models import load_model\n","\n","# Returns a compiled model identical to the previous one\n","model = load_model( 'custom_resent50_model.h5')\n","model.load_weights( 'model_weights_Img_aug_after_%s_epoch.h5' % epochs_passed )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1L3-JiPuHQ5a","outputId":"6a3a3cb4-d6c5-4c53-eb70-c5b686fd633c","colab":{"base_uri":"https://localhost:8080/","height":428}},"source":["model.compile(loss='categorical_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","\n","nb_epoch = 10 # epoch 11-20\n","# Fit the model on the batches generated by datagen.flow().\n","model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),\n","                    steps_per_epoch=X_train.shape[0] // batch_size,\n","                    validation_data=(X_test, Y_test),\n","                    epochs=nb_epoch, verbose=1, max_q_size=100,\n","                    callbacks=[lr_reducer, early_stopper, csv_logger])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., steps_per_epoch=200, validation_data=(array([[[..., epochs=10, verbose=1, callbacks=[<keras.ca..., max_queue_size=100)`\n","  # This is added back by InteractiveShellApp.init_path()\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/10\n","200/200 [==============================] - 311s 2s/step - loss: 3.2031 - acc: 0.4109 - val_loss: 3.6135 - val_acc: 0.3344\n","Epoch 2/10\n","200/200 [==============================] - 284s 1s/step - loss: 3.1008 - acc: 0.4286 - val_loss: 3.5681 - val_acc: 0.3448\n","Epoch 3/10\n","200/200 [==============================] - 283s 1s/step - loss: 3.0402 - acc: 0.4400 - val_loss: 3.5631 - val_acc: 0.3517\n","Epoch 4/10\n","200/200 [==============================] - 286s 1s/step - loss: 2.9875 - acc: 0.4492 - val_loss: 3.5427 - val_acc: 0.3522\n","Epoch 5/10\n","200/200 [==============================] - 284s 1s/step - loss: 2.9247 - acc: 0.4614 - val_loss: 3.5898 - val_acc: 0.3516\n","Epoch 6/10\n","200/200 [==============================] - 283s 1s/step - loss: 2.8781 - acc: 0.4735 - val_loss: 3.6709 - val_acc: 0.3428\n","Epoch 7/10\n","200/200 [==============================] - 283s 1s/step - loss: 2.8397 - acc: 0.4792 - val_loss: 3.3663 - val_acc: 0.3853\n","Epoch 8/10\n","200/200 [==============================] - 282s 1s/step - loss: 2.7899 - acc: 0.4911 - val_loss: 3.4387 - val_acc: 0.3875\n","Epoch 9/10\n","200/200 [==============================] - 284s 1s/step - loss: 2.7637 - acc: 0.4954 - val_loss: 3.5677 - val_acc: 0.3743\n","Epoch 10/10\n","200/200 [==============================] - 283s 1s/step - loss: 2.7335 - acc: 0.5033 - val_loss: 3.5918 - val_acc: 0.3686\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f01ace26470>"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"EgqYmAhyj26X"},"source":["epochs_passed = 20\n","model.save_weights( 'model_weights_Img_aug_after_%s_epoch.h5' % epochs_passed )\n","model.save( 'custom_resent50_model.h5' )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4qi7WYC_kFp1","outputId":"1de0f6a5-078b-4314-a3da-2d6381d3194a","colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["from keras.models import load_model\n","epochs_passed = 20\n","\n","# Returns a compiled model identical to the previous one\n","model = load_model( 'custom_resent50_model.h5')\n","model.load_weights( 'model_weights_Img_aug_after_%s_epoch.h5' % epochs_passed )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JzJzQl3IkGm0","outputId":"54e403ef-15cc-4aca-ade2-539edc8ab896","colab":{"base_uri":"https://localhost:8080/","height":428}},"source":["model.compile(loss='categorical_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","\n","nb_epoch = 10 # epoch 21-30\n","# Fit the model on the batches generated by datagen.flow().\n","model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),\n","                    steps_per_epoch=X_train.shape[0] // batch_size,\n","                    validation_data=(X_test, Y_test),\n","                    epochs=nb_epoch, verbose=1, max_q_size=100,\n","                    callbacks=[lr_reducer, early_stopper, csv_logger])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., steps_per_epoch=200, validation_data=(array([[[..., epochs=10, verbose=1, callbacks=[<keras.ca..., max_queue_size=100)`\n","  # This is added back by InteractiveShellApp.init_path()\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/10\n","200/200 [==============================] - 313s 2s/step - loss: 2.7222 - acc: 0.5083 - val_loss: 3.4446 - val_acc: 0.3883\n","Epoch 2/10\n","200/200 [==============================] - 280s 1s/step - loss: 2.6477 - acc: 0.5233 - val_loss: 3.5928 - val_acc: 0.3657\n","Epoch 3/10\n","200/200 [==============================] - 281s 1s/step - loss: 2.6144 - acc: 0.5319 - val_loss: 3.4228 - val_acc: 0.3854\n","Epoch 4/10\n","200/200 [==============================] - 278s 1s/step - loss: 2.5891 - acc: 0.5370 - val_loss: 3.4009 - val_acc: 0.4019\n","Epoch 5/10\n","200/200 [==============================] - 280s 1s/step - loss: 2.5640 - acc: 0.5431 - val_loss: 3.4203 - val_acc: 0.3932\n","Epoch 6/10\n","200/200 [==============================] - 281s 1s/step - loss: 2.5398 - acc: 0.5482 - val_loss: 3.5287 - val_acc: 0.3887\n","Epoch 7/10\n","200/200 [==============================] - 280s 1s/step - loss: 2.5110 - acc: 0.5564 - val_loss: 3.5288 - val_acc: 0.3833\n","Epoch 8/10\n","200/200 [==============================] - 280s 1s/step - loss: 2.4860 - acc: 0.5638 - val_loss: 3.3098 - val_acc: 0.4300\n","Epoch 9/10\n","200/200 [==============================] - 279s 1s/step - loss: 2.4649 - acc: 0.5680 - val_loss: 3.4218 - val_acc: 0.4083\n","Epoch 10/10\n","200/200 [==============================] - 278s 1s/step - loss: 2.4491 - acc: 0.5726 - val_loss: 3.4574 - val_acc: 0.4021\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7feba56be278>"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"WiuRvKmMKaKL"},"source":["epochs_passed = 30\n","model.save_weights( 'model_weights_Img_aug_after_%s_epoch.h5' % epochs_passed )\n","model.save( 'custom_resent50_model.h5' )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Zl7JjCMKmAL","outputId":"c027080b-6e02-4fba-8a0d-5497d6c8bfec","colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["from keras.models import load_model\n","epochs_passed = 30\n","\n","# Returns a compiled model identical to the previous one\n","model = load_model( 'custom_resent50_model.h5')\n","model.load_weights( 'model_weights_Img_aug_after_%s_epoch.h5' % epochs_passed )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ieskAuuQLCfL","outputId":"fe9a005c-1086-43c3-9eac-9566064cd5c9","colab":{"base_uri":"https://localhost:8080/","height":649}},"source":["model.compile(loss='categorical_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","\n","nb_epoch = 20 # epoch 31-50\n","# Fit the model on the batches generated by datagen.flow().\n","model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),\n","                    steps_per_epoch=X_train.shape[0] // batch_size,\n","                    validation_data=(X_test, Y_test),\n","                    epochs=nb_epoch, verbose=1, max_q_size=100,\n","                    callbacks=[lr_reducer, early_stopper, csv_logger])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., steps_per_epoch=200, validation_data=(array([[[..., epochs=20, verbose=1, callbacks=[<keras.ca..., max_queue_size=100)`\n","  # This is added back by InteractiveShellApp.init_path()\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/20\n","200/200 [==============================] - 312s 2s/step - loss: 2.4577 - acc: 0.5752 - val_loss: 3.3310 - val_acc: 0.4151\n","Epoch 2/20\n","200/200 [==============================] - 280s 1s/step - loss: 2.3981 - acc: 0.5879 - val_loss: 3.4938 - val_acc: 0.4039\n","Epoch 3/20\n","200/200 [==============================] - 278s 1s/step - loss: 2.3730 - acc: 0.5941 - val_loss: 3.5969 - val_acc: 0.3922\n","Epoch 4/20\n","200/200 [==============================] - 278s 1s/step - loss: 2.3490 - acc: 0.6011 - val_loss: 3.6004 - val_acc: 0.3967\n","Epoch 5/20\n","200/200 [==============================] - 280s 1s/step - loss: 2.3398 - acc: 0.6040 - val_loss: 3.4154 - val_acc: 0.4206\n","Epoch 6/20\n","200/200 [==============================] - 279s 1s/step - loss: 2.3191 - acc: 0.6106 - val_loss: 3.3694 - val_acc: 0.4399\n","Epoch 7/20\n","200/200 [==============================] - 280s 1s/step - loss: 1.9283 - acc: 0.7093 - val_loss: 2.9221 - val_acc: 0.5035\n","Epoch 8/20\n","200/200 [==============================] - 277s 1s/step - loss: 1.7692 - acc: 0.7434 - val_loss: 2.9753 - val_acc: 0.5000\n","Epoch 9/20\n","200/200 [==============================] - 269s 1s/step - loss: 1.6890 - acc: 0.7602 - val_loss: 2.9595 - val_acc: 0.5028\n","Epoch 10/20\n","200/200 [==============================] - 232s 1s/step - loss: 1.6167 - acc: 0.7748 - val_loss: 2.9868 - val_acc: 0.5057\n","Epoch 11/20\n","200/200 [==============================] - 232s 1s/step - loss: 1.5674 - acc: 0.7832 - val_loss: 3.0460 - val_acc: 0.4963\n","Epoch 12/20\n","200/200 [==============================] - 232s 1s/step - loss: 1.5219 - acc: 0.7915 - val_loss: 3.0250 - val_acc: 0.4962\n","Epoch 13/20\n","200/200 [==============================] - 232s 1s/step - loss: 1.3456 - acc: 0.8445 - val_loss: 2.9447 - val_acc: 0.5149\n","Epoch 14/20\n","200/200 [==============================] - 233s 1s/step - loss: 1.2760 - acc: 0.8623 - val_loss: 2.9518 - val_acc: 0.5126\n","Epoch 15/20\n","200/200 [==============================] - 233s 1s/step - loss: 1.2328 - acc: 0.8742 - val_loss: 2.9595 - val_acc: 0.5160\n","Epoch 16/20\n","200/200 [==============================] - 233s 1s/step - loss: 1.1964 - acc: 0.8817 - val_loss: 2.9875 - val_acc: 0.5123\n","Epoch 17/20\n","200/200 [==============================] - 233s 1s/step - loss: 1.1674 - acc: 0.8893 - val_loss: 3.0070 - val_acc: 0.5089\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yzmwIe6gg2zT","outputId":"88e7d259-d6f0-48c7-d4a5-b65cac13f0a8","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["epochs_passed = 47\n","model.save_weights( 'model_weights_Img_aug_after_%s_epoch.h5' % epochs_passed )\n","model.save( 'custom_resent50_model.h5' )\n","print(\"Model and weights after %s epochs have been saved\" % epochs_passed )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model and weights after 47 epochs have been saved\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3L2Bky9OhJVn","outputId":"ef90c03e-38df-491b-b54a-788d94f4c3e6","colab":{"base_uri":"https://localhost:8080/","height":462}},"source":["nb_epoch = 13 # epoch 47-60\n","# Fit the model on the batches generated by datagen.flow().\n","model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),\n","                    steps_per_epoch=X_train.shape[0] // batch_size,\n","                    validation_data=(X_test, Y_test),\n","                    epochs=nb_epoch, verbose=1, max_q_size=100,\n","                    callbacks=[lr_reducer, early_stopper, csv_logger])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., steps_per_epoch=200, validation_data=(array([[[..., epochs=13, verbose=1, callbacks=[<keras.ca..., max_queue_size=100)`\n","  import sys\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/13\n","200/200 [==============================] - 232s 1s/step - loss: 1.1088 - acc: 0.9068 - val_loss: 2.9980 - val_acc: 0.5161\n","Epoch 2/13\n","200/200 [==============================] - 232s 1s/step - loss: 1.0894 - acc: 0.9118 - val_loss: 3.0060 - val_acc: 0.5143\n","Epoch 3/13\n","200/200 [==============================] - 233s 1s/step - loss: 1.0760 - acc: 0.9154 - val_loss: 3.0285 - val_acc: 0.5154\n","Epoch 4/13\n","200/200 [==============================] - 233s 1s/step - loss: 1.0610 - acc: 0.9191 - val_loss: 3.0449 - val_acc: 0.5137\n","Epoch 5/13\n","200/200 [==============================] - 233s 1s/step - loss: 1.0486 - acc: 0.9216 - val_loss: 3.0565 - val_acc: 0.5128\n","Epoch 6/13\n","200/200 [==============================] - 233s 1s/step - loss: 1.0363 - acc: 0.9252 - val_loss: 3.0628 - val_acc: 0.5115\n","Epoch 7/13\n","200/200 [==============================] - 233s 1s/step - loss: 1.0210 - acc: 0.9302 - val_loss: 3.0554 - val_acc: 0.5146\n","Epoch 8/13\n","200/200 [==============================] - 234s 1s/step - loss: 1.0124 - acc: 0.9320 - val_loss: 3.0582 - val_acc: 0.5125\n","Epoch 9/13\n","200/200 [==============================] - 234s 1s/step - loss: 1.0103 - acc: 0.9320 - val_loss: 3.0648 - val_acc: 0.5134\n","Epoch 10/13\n","200/200 [==============================] - 233s 1s/step - loss: 1.0040 - acc: 0.9338 - val_loss: 3.0650 - val_acc: 0.5135\n","Epoch 11/13\n","200/200 [==============================] - 233s 1s/step - loss: 1.0023 - acc: 0.9339 - val_loss: 3.0657 - val_acc: 0.5135\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f6e41c465c0>"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"iVa1WfL3hTNi","outputId":"1fdfc6c1-813b-4427-fbe7-4a7869cbcf44","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["epochs_passed = 60\n","model.save_weights( 'model_weights_Img_aug_after_%s_epoch.h5' % epochs_passed )\n","model.save( 'custom_resent50_model.h5' )\n","print(\"Model and weights after %s epochs have been saved\" % epochs_passed )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model and weights after 60 epochs have been saved\n"],"name":"stdout"}]}]}